{"cells":[{"cell_type":"markdown","metadata":{"id":"bqH5jRbX2YwZ"},"source":["# COMP2211 PA1: K-nearest Neighbors Regressor for Wine Quality Prediction\n","### Introduction\n","The wine industry is highly competitive, and winemakers rely on chemical attributes like acidity, pH levels, residual sugar, and alcohol content to assess and predict wine quality. Accurate prediction is crucial for making informed production decisions and maintaining a good reputation.\n","\n","### Task Overview\n","In this assignment, we will implement a K-Nearest Neighbors (KNN) regressor from scratch with Numpy to predict wine quality based on these attributes. Through this assignment, we will gain hands-on experience in data preprocessing, KNN model building, and model evaluation. Good luck and enjoy the journey!\n"]},{"cell_type":"markdown","metadata":{"id":"VT-kxulI837Y"},"source":["## Data Description\n","Our source dataset is winequality-white.csv, one of the [Wine Quality datasets](http://archive.ics.uci.edu/dataset/186/wine+quality) from UCI Machine Learning Repository (Note: you can download the dataset in the PA1 web page). It is related to white Vinho Verde wine samples from northern Portugal, containing 4,898 instances with 12 attributes. The input variables are based on physicochemical tests and include the following attributes: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol. The output variable is the wine quality, represented by a score ranging from 0 to 10.\n","\n","Remarks:\n","Note that although in the original dataset, the wine quality is in integer, in this regression model project, we treat it as a continuous variable.\n","\n","Below is the list of column names, their roles and data type:\n","<center>\n","\n","Column Name                            | Role | Type\n","---------------------------------------|---------------|--------\n","fixed_acidity                          | Feature       | Continuous\n","volatile_acidit                        | Feature       | Continuous\n","citric_acid                            | Feature       | Continuous\n","residual_sugar                         | Feature       | Continuous\n","chlorides                              | Feature       | Continuous\n","free_sulfur_dioxide                    | Feature       | Continuous\n","total_sulfur_dioxide                   | Feature       | Continuous\n","density                                | Feature       | Continuous\n","pH                                     | Feature       | Continuous\n","sulphates                              | Feature       | Continuous\n","alcohol                                | Feature       | Continuous\n","quality                                | Target        | Integer\n","\n","</center>\n","\n","\n","But don't be scared by the data! To finish the assignment, you don't really need to understand these chemical attributes. Having a rough idea of the data structure is enough.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vIT2xENBySkK"},"source":["## Task 0: Set-up\n","Let's do all the basic set-up first!  \n","\n","Remarks: This part will not be graded."]},{"cell_type":"markdown","metadata":{"id":"7FerN9OW4NRE"},"source":["### Task 0.1: Import libraries\n","It's a good habit to import all libraries at the beginning of the code and it helps in the following aspects:\n","*   Readability and clarity\n","*   Avoiding namespace clashes\n","*   Dependency management\n","*   Consistency and convention\n","\n","Todo:  \n","Please import your libraries in the following cell.  \n","\n","Remarks:\n","1. We use [Numpy](https://numpy.org/) and [Pandas](https://pandas.pydata.org/) in this PA. You may also import other modules as long as they are part of the [Python Standard Library](https://docs.python.org/3/library/).  \n","2. You are NOT allowed to use any other external libraries/functions\n"," (especially any machine learning library, e.g., sklearn) in todo.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DWIyyARehnr"},"outputs":[],"source":["# task 0.1: import libraries\n","# todo start #\n","import numpy as np\n","import pandas as pd\n","# todo end #"]},{"cell_type":"markdown","metadata":{"id":"MXPkptPR5on3"},"source":["### Task 0.2: Read Dataset\n","Now you have the needed libraries in hand. Next, let's read the dataset from the source file to the project.  \n","\n","We assume you are working in Google Colab. One way to read a dataset in Google Colab:\n","1. Download the source file and put it on your Google Drive\n","2. Import the `drive` module from `google.colab` package\n","3. Run `drive.mount` to mount your Google Drive to the Colab notebook\n","4. Use `pandas.read_csv` to read the data from Google Drive and store the data in pandas DataFrame\n","\n","Todo:\n","Modify `YourFilePath` depending on the actual directory to read the data to this notebook.\n","\n","Remarks:  \n","You can check whether your data reading is successful by running the next cell. The shape should be (4892, 12). You can also see the first 5 rows of the dataset."]},{"cell_type":"markdown","metadata":{"id":"jMYW38ML0BTC"},"source":["## Task 1: Data Preprocessing\n","Before building our KNN model, let's get all the needed data. The raw data is usually not enough but needs to be preprocessed. There are many methods in the concept of data preprocessing.  \n","\n","In this section, we will explore two basic methods: [Data Splitting](https://www.geeksforgeeks.org/splitting-data-for-machine-learning-models/) and [Data Normalization](https://www.geeksforgeeks.org/what-is-data-normalization/).  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVXEVtX_0278"},"outputs":[],"source":["# Task 0.2: read dataset\n","if __name__ == '__main__':\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","# todo start #\n","# please modify YourFilePath\n","    data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/pa1/winequality-white.csv', delimiter=';')\n","# todo end #"]},{"cell_type":"markdown","metadata":{"id":"mA-BB9rp1EZg"},"source":["### Task 1.1: Data Splitting\n","Now `data` is our dataset with shape (4898, 12). We need to split the data for training and testing purposes. In this project, we let the training data contain the first 4000 rows and the testing data contain the remaining 898 rows. Also, we need to split the data into features and label (commonly called X and y in machine learning) arrays.\n","\n","Todo:  \n","Split the Pandas dataframe `data` and store in Numpy arrays `X_train`, `y_train`, `X_test`, `y_test`.  \n","\n","Remarks:  \n","1. This task would not be graded.\n","2. As we use many numpy functions later on, the output should be numpy arrays.\n","\n","Pandas functions you may use:  \n","`pandas.DataFrame.drop`, `pandas.DataFrame.iloc`, `pandas.DataFrame.to_numpy`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoS_jrDJzD1f","outputId":"964cfc7a-6e15-48cc-b42e-e61d68cb940d","executionInfo":{"status":"ok","timestamp":1697884401713,"user_tz":-480,"elapsed":7,"user":{"displayName":"龐蕊兒","userId":"08519029342978986803"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (4000, 11) and y_train shape: (4000,)\n","X_test shape: (898, 11) and y_test shape: (898,)\n"]}],"source":["# Task 1.1\n","if __name__ == '__main__':\n","    # todo start #\n","    vectors=data.to_numpy()\n","    X_train=vectors[:4000,:-1]\n","    X_test=vectors[4000:,:-1]\n","    y_train=vectors[:4000,-1]\n","    y_test=vectors[4000:,-1]\n","    # todo end #\n","    print(\"X_train shape: {} and y_train shape: {}\".format(X_train.shape, y_train.shape))\n","    print(\"X_test shape: {} and y_test shape: {}\".format(X_test.shape, y_test.shape))"]},{"cell_type":"markdown","metadata":{"id":"yjLNkLMj2t0n"},"source":["### Task 1.2: Data Normalization\n","\n","Normalization is a fundamental preprocessing step in machine learning. It helps to ensure fair treatment of features, facilitate efficient optimization, enhance interpretability, handle different measurement units, and mitigate the impact of outliers. By normalizing the data, we can improve the accuracy and reliability of machine learning models.  \n","\n","Let's introduce 2 common normalization methods: [**Min-Max Normalization** ](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)) and [ **Z-score Normalization**](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)). Suppose $X:(x_1, x_2, ..., x_n)$ is a column (corresponding to a feature), then\n","1. min-max normalization:  \n","$\\displaystyle X_{\\text{min-max-normalized}} = \\frac{X-X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$\n","2. z-score normalization:  \n","$\\displaystyle X_{\\text{Z-score-normalized}} = \\frac{X-\\mu_X}{\\sigma_X}$\n","\n","Todo:  \n","Please implement `min_max_normalization(input_array)` and `z_score_normalization(input_array)`.  \n","\n","Numpy functions you may use:  \n","`numpy.mean`, `numpy.min`, `numpy.max`, `numpy.std` ...\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGycdo2c3nBs"},"outputs":[],"source":["# Task 1.2.1\n","def min_max_normalization(input_array):\n","  # input_array: numpy array of shape (num_rows, num_features)\n","  # todo start #\n","  x_min=np.min(input_array,axis=0)\n","  x_max=np.max(input_array,axis=0)\n","  normalized_array=(input_array-x_min)/(x_max-x_min)\n","  # todo end #\n","  return normalized_array\n","  # normalized_array: numpy array of shape (num_rows, num_features)\n","\n","# Task 1.2.2\n","def z_score_normalization(input_array):\n","  # input_array: numpy array of shape (num_rows, num_features)\n","  # todo start #\n","  x_sd=np.std(input_array,ddof=0,axis=0)\n","  x_mean=np.mean(input_array,axis=0)\n","  normalized_array=(input_array-x_mean)/(x_sd)\n","  # todo end #\n","  return normalized_array\n","  # normalized_array: numpy array of shape (num_rows, num_features)\n"]},{"cell_type":"markdown","metadata":{"id":"_OyMc47l8GDr"},"source":["## Task 2: KNN Model\n","Now, the training data and testing data are ready. Let's build the KNN model!  \n","\n","In this session, we break down the KNN model into the following functional parts:\n","1. Distance Calculation\n","2. K Nearest Neighbors Finding\n","3. Prediction Generation\n"]},{"cell_type":"markdown","metadata":{"id":"1C0EGRys8Jz4"},"source":["### Task 2.1: Distance Calculation\n","\n","In KNN, distance calculation plays an important role as we need to find the k nearest neighbors to make the prediction. Here, we introduce 2 common distance calculation methods: [**Euclidean Distance**](https://en.wikipedia.org/wiki/Euclidean_distance) and [**Manhattan Distance**](https://en.wikipedia.org/wiki/Taxicab_geometry). Suppose we are calculating the distance between $X:(x_1, x_2, ... ,x_n)$ and $Y:(y_1, y_2, ... y_n)$ in n-dimensional space.\n","\n","1. Euclidean Distance:  \n","$\\displaystyle d(X, Y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$\n","2. Manhattan Distance:  \n","$\\displaystyle d(X, Y) = \\sum_{i=1}^{n} | x_i - y_i |$\n","\n","Todo:  \n","Please implement `euclidean_distance(X_train, X_test)` and `manhattan_distance(X_train, X_test)`.  \n","\n","Numpy functions you may use:  \n","`numpy.expand_dims`, `numpy.sqrt`, `numpy.sum` ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"874Wvw6V7LP2"},"outputs":[],"source":["# Task 2.1.1\n","def euclidean_distance(X_train, X_test):\n","  # X_train: numpy array of shape (num_rows_train, num_features)\n","  # X_test: numpy array of shape (num_rows_test, num_features)\n","  # todo start #\n","  xtrain_i=np.expand_dims(X_train,axis=0)\n","  xtest_i=np.expand_dims(X_test,axis=1)\n","  distance=np.sqrt(np.sum((xtrain_i-xtest_i)**2,axis=2))\n","  # todo end #\n","  return distance\n","  # distance: numpy array of shape (num_rows_test, num_rows_train)\n","\n","# Task 2.1.2\n","def manhattan_distance(X_train, X_test):\n","  # X_train: numpy array of shape (num_rows_train, num_features)\n","  # X_test: numpy array of shape (num_rows_test, num_features)\n","  # todo start #\n","  xtrain_i=np.expand_dims(X_train,axis=0)\n","  xtest_i=np.expand_dims(X_test,axis=1)\n","  distance=np.sum(abs(xtrain_i-xtest_i),axis=2)\n","  # todo end #\n","  return distance\n","  # distance: numpy array of shape (num_rows_test, num_rows_train)"]},{"cell_type":"markdown","metadata":{"id":"NqB3RORp9tmy"},"source":["### Task 2.2: Find K Nearest Neighbors\n","\n","Now we have the distance calculation functions; the next step is to find the k nearest neighbors for each test point.  \n","\n","Todo:  \n","Please implement `find_k_nearest_neighbor(distance, y_train, k)`  \n","\n","Remarks:\n","1. In case there is a tie, which means more than 1 training points share the same distance between a testing point, we consider the training point with smaller index as smaller (Can search the concept of stable sort).\n","\n","\n","Numpy functions you may use:  \n","`numpy.argsort`, `numpy.take`, `numpy.take_along_axis` ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBNCINxrx7lM"},"outputs":[],"source":["def find_k_nearest_neighbor(distance, y_train, k):\n","  # distance: numpy array of shape (num_rows_test, num_rows_train), return value from previous distance functions\n","  # y_train: numpy array of shape (num_rows_train, ),  the labels of training data\n","  # k: integer, k in \"K-nearest neighbors\"\n","  # todo start #\n","  k_indices=np.argsort(distance,axis=1)[:,:k]\n","  y_neighbor=np.take(y_train,k_indices)\n","  distance_neighbor=np.take_along_axis(distance,k_indices,axis=1)\n","\n","  # todo end #\n","  return y_neighbor, distance_neighbor\n","  # y_neighbor: numpy array of shape (num_rows_test, k), the labels of the k nearest neighbors of each test point\n","  # distance_neighbor: numpy array of shape (num_rows_test, k),  the distance between each test point and its k nearest neighbors"]},{"cell_type":"markdown","metadata":{"id":"Neeuk6gjA_xs"},"source":["### Task 2.3: Weighted Average Prediction\n","\n","In weighted average prediction, each data point's contribution to the final prediction is weighted based on its importance or relevance. The weights can be assigned manually or determined through a learning algorithm. Higher weights indicate higher importance (we set the weights manually here). The final prediction is obtained by taking the weighted average of the predictions made on each data point.\n","\n","Target:\n","Suppose the labels of k nearest neighbors of a test point are $Y:(y_1, y_2, ..., y_k)$, and the manually assigned weights are $W:(w_1, w_2, ..., w_k)$. Then the prediction value of this point should be\n","\n","$$\n","  y_{\\text{pred}} = \\frac{y_1 w_1 + \\cdots + y_k w_k}{w_1 + \\cdots + w_k}\n"," = \\frac{\\displaystyle \\sum_{i=1}^{k} y_i w_i}{\\displaystyle \\sum_{i=1}^{k} w_i}\n","$$\n","Todo:\n","Please implement `weighted_average_predict(y_neighbor, weights=None)`\n","\n","Remarks:\n","1. the parameter `weights` here is optional. If no `weights` array is passed into the function, then we should treat each of k nearest neighbors equally.  \n","\n","Numpy functions you may use:  \n","`numpy.expand_dims`, `numpy.sum` ...\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"du9q2c-9yM9K"},"outputs":[],"source":["def weighted_average_predict(y_neighbor, weights=None):\n","  # y_neighbor: numpy array of shape (num_rows_test, k), the labels of the k nearest neighbors of each test point\n","  # weights: numpy array of shape (k, ), controls the contribution of each near enighbor\n","  # todo start #\n","\n","  if weights is None:\n","    weights = np.ones((y_neighbor.shape[1],))\n","  prediction = np.sum(y_neighbor*weights,axis=1) / np.sum(weights)\n","\n","  # todo end #\n","  return prediction\n","  # prediction: numpy array of shape (num_rows_test, ), the weighted average prediction for each test point"]},{"cell_type":"markdown","metadata":{"id":"koWDA4QcB6sg"},"source":["### Task 2.4: Distance-based Prediction\n","\n","Distance-based weighted prediction assigns weights to data points based on their proximity or similarity to the query point. The idea is that closer data points are more likely to influence the prediction more than those farther away. Here, we use a common method: let the weights are inversely proportional to the distance from the query point.\n","\n","Target:\n","Suppose the labels of k nearest neighbors of a test point are $Y:(y_1, y_2, ..., y_k)$, and the distances between each neighbor and the test point are $D:(d_1, d_2, ..., d_k)$. Then\n","$\\displaystyle y_{\\text{pred}} = \\frac{\\sum_{i=1}^{k} y_iw_i}{\\sum_{i=1}^{k} w_i} $ where $\\displaystyle w_i = \\frac{1}{d_i + \\varepsilon}$.  \n","Notice we use $\\varepsilon$ here to avoid division by zero problem.\n","\n","Todo:\n","Please implement `distance_based_predict(y_neighbor, distance_neighbor, epsilon=1)`\n","\n","Numpy functions you may use:  \n","`numpy.sum` ...\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSNM8BHKyQxL"},"outputs":[],"source":["def distance_based_predict(y_neighbor, distance_neighbor, epsilon):\n","  # y_neighbor: numpy array of shape (num_rows_test, k), the labels of the k nearest neighbors of each test point\n","  # distance_neighbor, numpy array of shape (num_rows_test, k), the distance between the each k nearest neighbor and test point\n","  # epsilon: positive number, to avoid dividing by zero problem\n","  # todo start #\n","  weights = 1/(distance_neighbor + epsilon)\n","  prediction = np.sum(y_neighbor*weights,axis=1) / np.sum(weights,axis=1)\n","\n","  # todo end #\n","  return prediction\n","  # prediction: numpyt array of shape (num_rows_test, ), the distance-based prediction for each test point"]},{"cell_type":"markdown","metadata":{"id":"07QwcMhf9xQ1"},"source":["## Task 3: Metric Analyzer\n","\n","Using appropriate metrics to analyze machine learning models is of utmost importance as it enables quantitative performance assessment, facilitates model comparison and provides valuable insights into the model's effectiveness, aiding in informed decision-making and continuous improvement of the learning algorithms. In this task, we introduce 3 metrics.  \n","\n","Suppose $A:(a_1, a_2, ..., a_n)$ is actual labels and $P:(p_1, p_2, ... , p_n)$ is predicted labels.\n","The 3 metrics here to analyze the prediction quality are:\n","1.   [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error): $\\displaystyle \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n}{| a_i - p_i |}$\n","\n","2.   [Mean Square Error](https://en.wikipedia.org/wiki/Mean_squared_error): $\\displaystyle \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n {(a_i - p_i)^2}$\n","\n","3.   [Mean Absolute Percentage Error](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error): $\\displaystyle \\text{MAPE} = \\frac{1}{n}\\sum_{i=1}^{n}{\\lvert \\frac{a_i - p_i}{a_i} \\rvert}$\n","\n","Todo:  \n","Please implement `metric_analyze(y_true, y_pred)`\n","\n","Numpy functions you may use:  \n","`numpy.mean`, `numpy.min`, `numpy.absolute`...\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFkov6GgZvrd"},"outputs":[],"source":["def metric_analyze(y_true, y_pred):\n","  # y_true: numpy array of shape (num_rows_test, )\n","  # y_pred: numpy array of shape (num_rows_test, )\n","  # Task 3: metric calculation\n","  # todo start #\n","  mae = np.mean(np.absolute(y_true - y_pred))\n","  mse = np.mean((y_true - y_pred)**2)\n","  mape = np.mean(np.absolute((y_true - y_pred) / y_true))\n","\n","  # todo end #\n","  return mae, mse, mape\n","  # mae, mse, mape: number, the metric value"]},{"cell_type":"markdown","metadata":{"id":"BJkbP0S-AICP"},"source":["## Task 4: D-Fold Cross-validation\n","\n","[D-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) is a widely used technique in machine learning for evaluating model performance. It involves dividing the dataset into k subsets or folds, training the model D times using different folds as the test set, and the rest as the training set. By rotating the folds as the test set, D-fold cross-validation provides a more reliable estimate of the model's generalization ability. The performance metrics from each iteration are then averaged to assess the model's effectiveness. This approach is valuable for model evaluation, hyperparameter tuning, and comparing different algorithms.  \n","\n","[Good explanation](https://scikit-learn.org/stable/modules/cross_validation.html)  \n","[Video introduction](https://www.youtube.com/watch?v=TIgfjmp-4BA&ab_channel=Udacity)"]},{"cell_type":"markdown","metadata":{"id":"-Ig0ksdrFEGJ"},"source":["### Task 4.1: Split D Folds\n","To do D-fold cross-validation, we need the D folds of training and testing data. We can first divide the original dataset into k equal-sized parts. Each part represents a distinct subset of the data and is used as training and testing data during cross-validation. Specifically, each fold serves as the test set once while the remaining D-1 folds are used for training.\n","\n","Todo:  \n","Please implement `split_d_fold(X, y, d)`\n","\n","Remarks:\n","1. To distinguish with \"k\" in \"k nearest neighbor\", we use \"d\" to represent the number of folds in this task.\n","2. In theory, it's better to shuffle the data before splitting. But we don't do here for consistent behaviour.\n","3. In this task, you should adopt a direct and simple splitting method: Suppose the input contains $n=m*d+r$ records (r=n%d), the sizes of test folds should be: $(m+1, ..., m+1, m, ..., m)$, where m+1 appears r times. And we pick out the test sets from the beginning of the array.  \n","For example, if the data is [a, b, c, d, e, f, g, h] and d is 3, 8 = 3*2 + 2, so the sizes of test folds are (3, 3, 2). Then according to our rule, the test folds are [a,b,c], [d,e,f], [g,h].  \n","4. The order of training folds and test folds matters and should be corresponding. This means the i-th training fold and the i-th test fold should be able to be combined into one original data set. You must follow this rule.\n","\n","Numpy functions you may use:  \n","`numpy.array_split`, `numpy.concatenate` ...\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gi0nnGelywAG"},"outputs":[],"source":["def split_d_fold(X, y, d):\n","  # X: numpy array of shape (num_rows, num_features), the feature data\n","  # y: numpy array of shape (num_rows, ), the label data\n","  # d: integer, number of folds\n","  data = np.concatenate((X, y[:, np.newaxis]), axis=1) # for better data structure\n","  # todo start #\n","  splits = np.array_split(data, d)      # can we assume that d must smaller or equal to num_rows????\n","\n","  train_d_folds = []\n","  test_d_folds = []\n","\n","  for i in range(d):\n","    test_fold = splits[i]\n","    train_folds = np.concatenate(splits[:i] + splits[i+1:], axis=0)\n","    train_d_folds.append(train_folds)\n","    test_d_folds.append(test_fold)\n","\n","\n","  # todo end #\n","  return train_d_folds, test_d_folds\n","  # train_d_folds: a pyhon list of length d, each entry is a training fold, each fold contains both features and labels\n","  # test_d_folds: a python list of length d, each entry is a testing fold, each fold contains both features and labels\n","  # the the i-th entry of train_d_folds and test_d_folds are corresponding"]},{"cell_type":"markdown","metadata":{"id":"9BHKs7Fp_N44"},"source":["### Task 4.2: Cross-Validation\n","Now, we have the training and test folds in hand. Let's use these folds to validate the performance of knn model composed with functional parts.\n","\n","Todo:  \n","Please implement `cross_validate`, a skeleton code is provided for you.\n","\n","Recommanded steps:\n","1. Read and understand the provided code.\n","2. Generate X_train, X_test, y_train, y_test for each round.\n","3. Make prediction with the KNN model composed with proper implemented functional parts.\n","4. Analyze the prediction with proper metric and store the result in scores.\n","\n","Remarks:\n","1. The KNN models to be validated here use Euclidean distance and weighted-average prediction that treat every neighbor equally. The only difference is the k of each model.\n","2. MSE is used as metric.\n","3. No external function is allowed.\n","4. During grading, we will pass the training and test folds (not generating from your `split_d_fold` function). And we only care about the return mean_scores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kmzl7i4Iy2Cs"},"outputs":[],"source":["def cross_validate(train_d_folds, test_d_folds, k_list):\n","  # train_d_folds: a pyhon list of length d, each entry is a training fold (numpy array)\n","  # test_d_folds: a python list of length d, each entry is a test fold (numpy array)\n","  # k_list: a python list, contains the k(s) to be validated\n","  # the i-th train_fold and test_fold are corresponding\n","  scores = np.zeros((len(k_list), len(train_d_folds)))\n","  # scores: a numpy array of shape (len(k_list), len(d_folds)), contains the metric for specific k and fold\n","  for k_index, k in enumerate(k_list):\n","    for fold in range(len(train_d_folds)):\n","      # Generate X_train, X_test, y_train, y_test for each round\n","      # todo start #\n","      X_train = train_d_folds[fold][:,:-1]\n","      y_train = train_d_folds[fold][:,-1]\n","      X_test = test_d_folds[fold][:,:-1]\n","      y_test = test_d_folds[fold][:,-1]\n","      # todo end #\n","\n","      # compose your knn model such that it use Euclidean distance and equally weighted average prediction\n","      # make prediction and analyze the metric and stores the metric to scores\n","      # todo start #\n","      distances = euclidean_distance(X_train, X_test)\n","      y_neighbor, distance_neighbor = find_k_nearest_neighbor(distances, y_train, k)\n","      predictions = weighted_average_predict(y_neighbor)\n","      mae, mse, mape = metric_analyze(y_test, predictions)\n","      scores[k_index, fold] = mse\n","      # todo end #\n","  mean_scores = np.mean(scores, axis=1, keepdims=False)\n","  # only cares about the mean value of the scores for each k, this avoid the possible affect from order of folds\n","  return mean_scores\n","  # mean_scores: numpy array of shape(len(k_list), ), the mean score for each k"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDER2cFEchBv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697888142708,"user_tz":-480,"elapsed":6417,"user":{"displayName":"龐蕊兒","userId":"08519029342978986803"}},"outputId":"5c8137fb-cdae-4100-f924-b9f2aa7180f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["cross-validation scores: \n"," [0.60793388 0.59584889 0.59544321]\n"]}],"source":["if __name__ == '__main__':\n","    normalized_X_train = min_max_normalization(X_train)\n","    normalized_X_test = min_max_normalization(X_test)\n","    train_d_folds, test_d_folds = split_d_fold(normalized_X_train, y_train, 5)\n","    print(\"cross-validation scores: \\n\", cross_validate(train_d_folds, test_d_folds, [11,15,19]))"]},{"cell_type":"markdown","metadata":{"id":"F3sN_wAPuW4E"},"source":["If you implement all relevant parts correctly, you should get the same output:  \n","cross validate scores:  \n"," [0.60793388 0.59584889 0.59544321]"]},{"cell_type":"markdown","metadata":{"id":"kOq1eNJg_OOK"},"source":["### Task 4.3: Find the Best K\n","Suppose we used `cross_validate` to get the mean score for each k in the k-list. Now it's time to decide which k is the best. Suppose we first care about the prediction quality (which means score, in our case, the lower score means better prediction). When the prediction qualities for 2 k are the same, we pick the smaller one to improve efficiency.\n","\n","Todo:  \n","Please implement `find_best_k`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vaHTRLSRy3wz"},"outputs":[],"source":["def find_best_k(k_list, mean_scores):\n","  # k_list: a python list, contains the k to be validated, order is not guaranteed\n","  # mean_scores: a numpy array of shape (len(k_list), ), the mean score for each k\n","  # todo start #\n","  mean_scores.sort()\n","  best_score = np.argmin(mean_scores)\n","  best_k = k_list[best_score]\n","\n","  # todo end\n","  return best_k\n","  # best_k: number, value of the smallest k that obtain best mean score in the cross-validation"]},{"cell_type":"markdown","metadata":{"id":"nhSxXCzq5mYu"},"source":["## Playground: Try out your model here\n","We provide some code only for your self-testing purposes. After you finish all tasks, you can try to run the testing code. If an error occurs, please go back and check your code carefully.\n","\n","This part will not be graded.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gn-9AY4K9KV3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697888603178,"user_tz":-480,"elapsed":9543,"user":{"displayName":"龐蕊兒","userId":"08519029342978986803"}},"outputId":"467e20a4-8284-4d63-c437-7f148caac2b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["(0.6391982182628062, 0.6494689760533048, 0.10751007529960759)\n","17\n"]}],"source":["if __name__ == '__main__':\n","    normalized_X_train = min_max_normalization(X_train)\n","    normalized_X_test = min_max_normalization(X_test)\n","    distance = euclidean_distance(normalized_X_train, normalized_X_test)\n","    y_neighbor, distance_neighbor = find_k_nearest_neighbor(distance, y_train, 11)\n","    y_pred = weighted_average_predict(y_neighbor)\n","    print(metric_analyze(y_test, y_pred))\n","    train_d_folds, test_d_folds = split_d_fold(normalized_X_train, y_train, 5)\n","    list_k = [11,13,15,17,19]\n","    print(find_best_k(list_k, cross_validate(train_d_folds, test_d_folds, list_k)))"]},{"cell_type":"markdown","metadata":{"id":"bo9g81SdpxfW"},"source":["### Optional Task:\n","Compare your KNN model with the standard KNN model in scikit-learn. Which one is better in terms of accuracy? What about efficiency? Think about the outcomes and discuss your ideas with others!\n","\n","Remarks:  \n","If you implement all functional parts correctly, and use euclidean_distance and equally weighted_average_predict to compose your KNN model, it should yield the same prediction with the default KNNRegressor :)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"3FeNwe4Fpvsr","outputId":"517a7c16-0334-4991-d71d-160e032e8215","executionInfo":{"status":"error","timestamp":1698054160385,"user_tz":-480,"elapsed":1580,"user":{"displayName":"龐蕊兒","userId":"08519029342978986803"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-746b5fc7dd33>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_analyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'normalized_X_train' is not defined"]}],"source":["if __name__ == '__main__':\n","    from sklearn.neighbors import KNeighborsRegressor\n","    knn = KNeighborsRegressor(n_neighbors = 11)\n","    knn.fit(normalized_X_train, y_train)\n","    y_pred = knn.predict(normalized_X_test)\n","    print(metric_analyze(y_test, y_pred))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}